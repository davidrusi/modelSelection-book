<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Quick start | Model selection with the modelSelection package</title>
  <meta name="description" content="An introduction to model selection illustrated with the modelSelection R package." />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Quick start | Model selection with the modelSelection package" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introduction to model selection illustrated with the modelSelection R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Quick start | Model selection with the modelSelection package" />
  
  <meta name="twitter:description" content="An introduction to model selection illustrated with the modelSelection R package." />
  

<meta name="author" content="David Rossell" />


<meta name="date" content="2025-09-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="background-bms.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="quick-start.html"><a href="quick-start.html"><i class="fa fa-check"></i><b>1</b> Quick start</a>
<ul>
<li class="chapter" data-level="1.1" data-path="quick-start.html"><a href="quick-start.html#linear-regression"><i class="fa fa-check"></i><b>1.1</b> Linear regression</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="quick-start.html"><a href="quick-start.html#l0-criteria"><i class="fa fa-check"></i><b>1.1.1</b> L0 criteria</a></li>
<li class="chapter" data-level="1.1.2" data-path="quick-start.html"><a href="quick-start.html#bayesian-model-selection"><i class="fa fa-check"></i><b>1.1.2</b> Bayesian model selection</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="quick-start.html"><a href="quick-start.html#logistic-regression"><i class="fa fa-check"></i><b>1.2</b> Logistic regression</a></li>
<li class="chapter" data-level="1.3" data-path="quick-start.html"><a href="quick-start.html#non-linear-effects-via-generalized-additive-models-gams"><i class="fa fa-check"></i><b>1.3</b> Non-Linear effects via Generalized Additive Models (GAMs)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="background-bms.html"><a href="background-bms.html"><i class="fa fa-check"></i><b>2</b> Background on Bayesian model selection and averaging</a>
<ul>
<li class="chapter" data-level="2.1" data-path="background-bms.html"><a href="background-bms.html#bms-simple-example"><i class="fa fa-check"></i><b>2.1</b> A simplest example</a></li>
<li class="chapter" data-level="2.2" data-path="background-bms.html"><a href="background-bms.html#bms-framework"><i class="fa fa-check"></i><b>2.2</b> General framework</a></li>
<li class="chapter" data-level="2.3" data-path="background-bms.html"><a href="background-bms.html#bms-modelprior"><i class="fa fa-check"></i><b>2.3</b> Prior on models</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="background-bms.html"><a href="background-bms.html#binomial-prior"><i class="fa fa-check"></i><b>2.3.1</b> Binomial prior</a></li>
<li class="chapter" data-level="2.3.2" data-path="background-bms.html"><a href="background-bms.html#beta-binomial-prior"><i class="fa fa-check"></i><b>2.3.2</b> Beta-Binomial prior</a></li>
<li class="chapter" data-level="2.3.3" data-path="background-bms.html"><a href="background-bms.html#complexity-prior"><i class="fa fa-check"></i><b>2.3.3</b> Complexity prior</a></li>
<li class="chapter" data-level="2.3.4" data-path="background-bms.html"><a href="background-bms.html#a-simple-example"><i class="fa fa-check"></i><b>2.3.4</b> A simple example</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="background-bms.html"><a href="background-bms.html#bms-coefprior"><i class="fa fa-check"></i><b>2.4</b> Prior on coefficients</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="background-bms.html"><a href="background-bms.html#bms-localprior"><i class="fa fa-check"></i><b>2.4.1</b> Local priors</a></li>
<li class="chapter" data-level="2.4.2" data-path="background-bms.html"><a href="background-bms.html#bms-nlp"><i class="fa fa-check"></i><b>2.4.2</b> Non-local priors</a></li>
<li class="chapter" data-level="2.4.3" data-path="background-bms.html"><a href="background-bms.html#bms-priorvar"><i class="fa fa-check"></i><b>2.4.3</b> Sensitivity to prior variance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="background-bms.html"><a href="background-bms.html#bms-computation"><i class="fa fa-check"></i><b>2.5</b> Computation</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="background-bms.html"><a href="background-bms.html#approximating-marginal-likelihoods"><i class="fa fa-check"></i><b>2.5.1</b> Approximating marginal likelihoods</a></li>
<li class="chapter" data-level="2.5.2" data-path="background-bms.html"><a href="background-bms.html#model-search"><i class="fa fa-check"></i><b>2.5.2</b> Model search</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="background-l0.html"><a href="background-l0.html"><i class="fa fa-check"></i><b>3</b> Background on L0 criteria</a>
<ul>
<li class="chapter" data-level="3.1" data-path="background-l0.html"><a href="background-l0.html#basics"><i class="fa fa-check"></i><b>3.1</b> Basics</a></li>
<li class="chapter" data-level="3.2" data-path="background-l0.html"><a href="background-l0.html#mcmc-for-model-search"><i class="fa fa-check"></i><b>3.2</b> MCMC for model search</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>4</b> Generalized linear models</a></li>
<li class="chapter" data-level="5" data-path="gam.html"><a href="gam.html"><i class="fa fa-check"></i><b>5</b> Generalized additive models</a></li>
<li class="chapter" data-level="6" data-path="eBayes.html"><a href="eBayes.html"><i class="fa fa-check"></i><b>6</b> Empirical Bayes for transfer learning</a></li>
<li class="chapter" data-level="7" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>7</b> Survival data</a></li>
<li class="chapter" data-level="8" data-path="ggm.html"><a href="ggm.html"><i class="fa fa-check"></i><b>8</b> Gaussian graphical models</a></li>
<li class="chapter" data-level="9" data-path="mixtures.html"><a href="mixtures.html"><i class="fa fa-check"></i><b>9</b> Gaussian mixture models</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model selection with the modelSelection package</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="quick-start" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Quick start<a href="quick-start.html#quick-start" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The main functions for regression-type models are <code>modelSelection</code> and <code>bestBIC</code> (along with companions like <code>bestEBIC</code>, <code>bestAIC</code> and <code>bestIC</code>).
Details are in subsequent sections. Below we illustrate how to obtain:</p>
<ul>
<li>Information criteria for all models (including those from MCMC exploration)</li>
<li>Posterior model probabilities</li>
<li>Marginal posterior inclusion probabilities<br />
</li>
<li>BMA point estimates and posterior intervals</li>
</ul>
<p>See <code>modelSelectionGGM</code> for Gaussian graphical models and <code>bfnormmix</code> for Gaussian mixture models.</p>
<div id="linear-regression" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Linear regression<a href="quick-start.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We simulate linear regression data
<span class="math display">\[
y_i = \sum_{j=1}^p x_{ij} \theta_j + \epsilon_i,
\]</span>
for <span class="math inline">\(p=3\)</span> covariates and <span class="math inline">\(i=1,\ldots,100\)</span> individuals.
We set regression coefficients <span class="math inline">\(\theta_1= 1\)</span>, <span class="math inline">\(\theta_2= 1\)</span>, <span class="math inline">\(\theta_3=0\)</span> and <span class="math inline">\(\epsilon_i \sim N(0,1)\)</span>, and the random number seed for reproducibility.
It is good practice to store the outcome and covariates into a data frame, as we do below.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="quick-start.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(mombf)</span>
<span id="cb1-2"><a href="quick-start.html#cb1-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb1-3"><a href="quick-start.html#cb1-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">100</span><span class="sc">*</span><span class="dv">3</span>), <span class="at">nrow=</span><span class="dv">100</span>, <span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb1-4"><a href="quick-start.html#cb1-4" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb1-5"><a href="quick-start.html#cb1-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">%*%</span> theta <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb1-6"><a href="quick-start.html#cb1-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span></code></pre></div>
<div id="l0-criteria" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> L0 criteria<a href="quick-start.html#l0-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>bestBIC</code> obtains the BIC for all models.
As usual in R we can use formulas like <code>y ~ X1 + X2 + X3</code> (provided the variables are stored in a data frame), or simply <code>y ~ .</code> to consider all the variables in the data (other than <code>y</code>) as covariates.
An intercept is added automatically, giving a total of 4 variables and <span class="math inline">\(2^4=16\)</span> possible models (the intercept can be removed by adding -1 to the formula, as usual). <code>bestBIC</code> enumerates these 16 models and finds the model with best (lowest) BIC (when there are many covariates, MCMC is used to explore the model space).
For our simulated data the selected model matches the data-generating truth, which only features the first two covariates.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="quick-start.html#cb2-1" tabindex="-1"></a>fit.bic <span class="ot">&lt;-</span> <span class="fu">bestBIC</span>(y <span class="sc">~</span> ., <span class="at">data=</span>df)</span></code></pre></div>
<pre><code>## Enumerating models...
##  Computing posterior probabilities
## 0%6%12%18%25%31%37%43%50%56%62%68%75%81%87%93% Done</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="quick-start.html#cb4-1" tabindex="-1"></a><span class="fu">print</span>(fit.bic)</span></code></pre></div>
<pre><code>## icfit object
## 
## Model with best BIC : X1 X2 
## 
## Use summary(), coef() and predict() to get inference for the top model
## Use coef(object$msfit) and predict(object$msfit) to get BMA estimates and predictions</code></pre>
<p>We list the BIC for the top 5 models (index 1 corresponds to the intercept, 2 to <code>x[,1]</code> and so on). We can also use standard functions like <code>summary</code> and <code>coef</code> to view the MLE for the best model, and <code>predict</code> to obtain predictions for new data.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="quick-start.html#cb6-1" tabindex="-1"></a>fit.bic<span class="sc">$</span>models[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,]</span></code></pre></div>
<pre><code>## # A tibble: 5 × 2
##   modelid   bic
##   &lt;chr&gt;   &lt;dbl&gt;
## 1 2,3      302.
## 2 2,3,4    307.
## 3 1,2,3    307.
## 4 1,2,3,4  311.
## 5 3        381.</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="quick-start.html#cb8-1" tabindex="-1"></a><span class="fu">summary</span>(fit.bic)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = f, family = family2glm(ms$family), data = data)
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## X1   1.1505     0.1022   11.26   &lt;2e-16 ***
## X2   1.1509     0.1006   11.44   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 1.06776)
## 
##     Null deviance: 371.43  on 100  degrees of freedom
## Residual deviance: 104.64  on  98  degrees of freedom
## AIC: 294.32
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="quick-start.html#cb10-1" tabindex="-1"></a><span class="fu">coef</span>(fit.bic)</span></code></pre></div>
<pre><code>##       X1       X2 
## 1.150549 1.150920</code></pre>
</div>
<div id="bayesian-model-selection" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Bayesian model selection<a href="quick-start.html#bayesian-model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A limitation of L0 criteria is that they ignore the uncertainty in the selected model, i.e. we’re not completely sure that it’s the correct one.
We use BMS to assess that uncertainty, in a Bayesian sense.
BMS requires setting a prior distribution on the models and on the parameters for each model.
For now, we run <code>modelSelection</code> with default priors (Beta-Binomial prior on the models, pMOM prior with default prior precision on the coefficients).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="quick-start.html#cb12-1" tabindex="-1"></a>priorCoef <span class="ot">&lt;-</span> <span class="fu">momprior</span>()</span>
<span id="cb12-2"><a href="quick-start.html#cb12-2" tabindex="-1"></a>priorDelta <span class="ot">&lt;-</span> <span class="fu">modelbbprior</span>()</span>
<span id="cb12-3"><a href="quick-start.html#cb12-3" tabindex="-1"></a>fit.bms <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y <span class="sc">~</span> ., <span class="at">data=</span>df,</span>
<span id="cb12-4"><a href="quick-start.html#cb12-4" tabindex="-1"></a>                     <span class="at">priorCoef=</span>priorCoef, </span>
<span id="cb12-5"><a href="quick-start.html#cb12-5" tabindex="-1"></a>                     <span class="at">priorDelta=</span>priorDelta)</span></code></pre></div>
<pre><code>## Enumerating models...
##  Computing posterior probabilities
## 0%6%12%18%25%31%37%43%50%56%62%68%75%81%87%93% Done</code></pre>
<p><code>postProb</code> shows posterior model probabilities (sorted decreasingly), and <code>coef</code> gives BMA point estimates, 0.95 posterior intervals, and marginal posterior inclusion probabilities <span class="math inline">\(P(\theta_j \neq 0 \mid y)\)</span>.
Below, <code>phi</code> refers to the error variance <span class="math inline">\(\sigma^2\)</span> (which is included with probability 1).
In our example, BMS selects the right covariates and assigns high posterior probability to that solution, as one would ideally wish.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="quick-start.html#cb14-1" tabindex="-1"></a><span class="fu">coef</span>(fit.bms)</span></code></pre></div>
<pre><code>##                estimate        2.5%      97.5%      margpp
## (Intercept) 0.007082034 -0.02658464 0.04089499 0.007366249
## X1          1.133309621  0.93331088 1.33480178 1.000000000
## X2          1.134404673  0.93919629 1.33501531 1.000000000
## X3          0.000366013  0.00000000 0.00000000 0.008254065
## phi         1.103715115  0.84213596 1.44604848 1.000000000</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="quick-start.html#cb16-1" tabindex="-1"></a><span class="fu">postProb</span>(fit.bms)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,]</span></code></pre></div>
<pre><code>##    modelid family           pp
## 7      2,3 normal 9.845428e-01
## 8    2,3,4 normal 8.090989e-03
## 15   1,2,3 normal 7.203173e-03
## 16 1,2,3,4 normal 1.630761e-04
## 3        3 normal 3.424188e-17</code></pre>
<p>Finally, we can use <code>predict</code> to obtain point predictions and 0.95 posterior predictive intervals.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="quick-start.html#cb18-1" tabindex="-1"></a>ypred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.bms)</span>
<span id="cb18-2"><a href="quick-start.html#cb18-2" tabindex="-1"></a><span class="fu">head</span>(ypred)</span></code></pre></div>
<pre><code>##         mean       2.5%       97.5%
## 1 -0.8928148 -1.1160111 -0.66885457
## 2 -0.2161415 -0.3514485 -0.08236455
## 3  1.3134407  1.0653356  1.56205993
## 4 -3.2261301 -3.6793885 -2.77249364
## 5 -0.4427614 -0.6498843 -0.23853199
## 6  0.7716784  0.6332783  0.90914325</code></pre>
</div>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Logistic regression<a href="quick-start.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For binary outcomes, we simply specify <code>family='binomial'</code> (and for Poisson we specify <code>family='poisson'</code>).
We first create a binary version of our simulated outcome.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="quick-start.html#cb20-1" tabindex="-1"></a>dfbin <span class="ot">&lt;-</span> <span class="fu">transform</span>(df, <span class="at">ybin =</span> (y <span class="sc">&gt;</span> <span class="dv">0</span>)) <span class="sc">|&gt;</span></span>
<span id="cb20-2"><a href="quick-start.html#cb20-2" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">!</span>y) <span class="co">#drop variable y</span></span></code></pre></div>
<p>We next use <code>bestBIC</code> and <code>modelSelection</code> as before. The selected model is still the correct one, but the posterior probability for (wrongly) including <code>x[,3]</code> is higher than in the linear regression data. This is intuitively expected, binary outcomes carry less information than Gaussian ones, so there is more uncertainty on the chosen model.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="quick-start.html#cb21-1" tabindex="-1"></a>fit2.bic <span class="ot">&lt;-</span> <span class="fu">bestBIC</span>(ybin <span class="sc">~</span> ., <span class="at">family=</span><span class="st">&#39;binomial&#39;</span>, <span class="at">data=</span>dfbin)</span></code></pre></div>
<pre><code>## Enumerating models...
##  Computing posterior probabilities
## 0%6%12%18%25%31%37%43%50%56%62%68%75%81%87%93% Done</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="quick-start.html#cb23-1" tabindex="-1"></a><span class="fu">print</span>(fit2.bic)</span></code></pre></div>
<pre><code>## icfit object
## 
## Model with best BIC : X1 X2 
## 
## Use summary(), coef() and predict() to get inference for the top model
## Use coef(object$msfit) and predict(object$msfit) to get BMA estimates and predictions</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="quick-start.html#cb25-1" tabindex="-1"></a>fit2.bms <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(ybin <span class="sc">~</span> ., <span class="at">data=</span>dfbin,</span>
<span id="cb25-2"><a href="quick-start.html#cb25-2" tabindex="-1"></a>                     <span class="at">priorCoef=</span>priorCoef,</span>
<span id="cb25-3"><a href="quick-start.html#cb25-3" tabindex="-1"></a>                     <span class="at">priorDelta=</span>priorDelta,</span>
<span id="cb25-4"><a href="quick-start.html#cb25-4" tabindex="-1"></a>                     <span class="at">family=</span><span class="st">&#39;binomial&#39;</span>)</span></code></pre></div>
<pre><code>## Enumerating models...
##  Computing posterior probabilities
## 0%6%12%18%25%31%37%43%50%56%62%68%75%81%87%93% Done</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="quick-start.html#cb27-1" tabindex="-1"></a><span class="fu">coef</span>(fit2.bms)</span></code></pre></div>
<pre><code>## Warning in hasPostSampling(object): Exact posterior sampling not implemented,
## using Normal approx instead</code></pre>
<pre><code>##               estimate       2.5%     97.5%       margpp
## (Intercept) 0.16323081 0.03275118 0.2669387 2.730669e-10
## X1          1.38893900 0.78354882 2.0102542 1.000000e+00
## X2          1.05744788 0.53310589 1.6067301 9.999136e-01
## X3          0.07043283 0.00000000 0.7391405 1.695233e-01</code></pre>
</div>
<div id="non-linear-effects-via-generalized-additive-models-gams" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Non-Linear effects via Generalized Additive Models (GAMs)<a href="quick-start.html#non-linear-effects-via-generalized-additive-models-gams" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Non-linear effects can be modeled via cubic splines using the <code>smooth</code> argument (the default is 9 knots, producing 5 columns in design matrix for each non-linear covariate).
When using the smooth argument we cannot use the <code>~ .</code> notation for including all covarites, rather we must list those for which we wish to include a non-linear effect (see the example below).
The effect of each covariate is decomposed as a linear part plus a deviation from linearity (which is forced to be orthogonal to the linear term). This is useful to identify covariates for which a linear effect is sufficient, and covariates for which there are non-linearities.
<code>modelSelection</code> considers 3 possibilities for each covariate: excluding it entirely, including only the linear effect, and including both linear and non-linear terms. For further details on this decomposition, see <span class="citation">(<a href="#ref-rossell:2021b">D. Rossell and Rubio 2021</a>)</span>.</p>
<p>The linear effect coefficients are displayed using the original variable names, and the non-linear coefficients with an <code>.s</code> appended. Here we have 5 columns coding for the non-linear effect, labelled as <code>.s1</code> though <code>.s5</code>.
In our example, there is strong evidence for (correctly) including the linear effect of <code>X1</code> and <code>X2</code>, excluding their non-linear effects, and exclusing <code>X3</code> entirely.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="quick-start.html#cb30-1" tabindex="-1"></a>fit.gam <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y <span class="sc">~</span> ., <span class="at">data=</span>df,</span>
<span id="cb30-2"><a href="quick-start.html#cb30-2" tabindex="-1"></a>                     <span class="at">smooth =</span> <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3,</span>
<span id="cb30-3"><a href="quick-start.html#cb30-3" tabindex="-1"></a>                     <span class="at">priorCoef=</span>priorCoef,</span>
<span id="cb30-4"><a href="quick-start.html#cb30-4" tabindex="-1"></a>                     <span class="at">priorDelta=</span>priorDelta, <span class="at">verbose=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="quick-start.html#cb31-1" tabindex="-1"></a><span class="fu">coef</span>(fit.gam)</span></code></pre></div>
<pre><code>## Warning in hasPostSampling(object): Exact posterior sampling not implemented,
## using Normal approx instead</code></pre>
<pre><code>##                  estimate        2.5%      97.5%       margpp
## (Intercept)  8.127021e-03 -0.01028342 0.02684467 7.499102e-03
## X1           1.140319e+00  1.02940753 1.25420735 1.000000e+00
## X2           1.139502e+00  1.03040279 1.24949508 1.000000e+00
## X3           1.579339e-04  0.00000000 0.00000000 8.545216e-03
## X1.s1       -1.088515e-04  0.00000000 0.00000000 4.026757e-04
## X1.s2        1.357836e-04  0.00000000 0.00000000 4.026757e-04
## X1.s3       -2.810730e-04  0.00000000 0.00000000 4.026757e-04
## X1.s4        2.847856e-04  0.00000000 0.00000000 4.026757e-04
## X1.s5        2.822071e-05  0.00000000 0.00000000 4.026757e-04
## X2.s1        4.295849e-03  0.00000000 0.00000000 4.296470e-03
## X2.s2        5.891295e-03  0.00000000 0.00000000 4.296470e-03
## X2.s3        2.200417e-03  0.00000000 0.00000000 4.296470e-03
## X2.s4        3.947671e-03  0.00000000 0.00000000 4.296470e-03
## X2.s5        1.376460e-03  0.00000000 0.00000000 4.296470e-03
## X3.s1       -1.142249e-04  0.00000000 0.00000000 1.855263e-05
## X3.s2        8.956766e-05  0.00000000 0.00000000 1.855263e-05
## X3.s3        7.400172e-05  0.00000000 0.00000000 1.855263e-05
## X3.s4        5.668310e-05  0.00000000 0.00000000 1.855263e-05
## X3.s5        1.327733e-04  0.00000000 0.00000000 1.855263e-05
## phi          1.093750e+00  0.82918219 1.43831078 1.000000e+00</code></pre>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-rossell:2021b" class="csl-entry">
———. 2021. <span>“Additive <span>B</span>ayesian Variable Selection Under Censoring and Misspecification.”</span> <em>Statistical Science</em> 38 (1): 13–29.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="background-bms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": null,
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
