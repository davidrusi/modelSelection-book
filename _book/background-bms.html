<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Background on Bayesian model selection and averaging | Model selection with the modelSelection package</title>
  <meta name="description" content="An introduction to model selection illustrated with the modelSelection R package." />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Background on Bayesian model selection and averaging | Model selection with the modelSelection package" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introduction to model selection illustrated with the modelSelection R package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Background on Bayesian model selection and averaging | Model selection with the modelSelection package" />
  
  <meta name="twitter:description" content="An introduction to model selection illustrated with the modelSelection R package." />
  

<meta name="author" content="David Rossell" />


<meta name="date" content="2025-09-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="quick-start.html"/>
<link rel="next" href="background-l0.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="quick-start.html"><a href="quick-start.html"><i class="fa fa-check"></i><b>1</b> Quick start</a>
<ul>
<li class="chapter" data-level="1.1" data-path="quick-start.html"><a href="quick-start.html#linear-regression"><i class="fa fa-check"></i><b>1.1</b> Linear regression</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="quick-start.html"><a href="quick-start.html#l0-criteria"><i class="fa fa-check"></i><b>1.1.1</b> L0 criteria</a></li>
<li class="chapter" data-level="1.1.2" data-path="quick-start.html"><a href="quick-start.html#bayesian-model-selection"><i class="fa fa-check"></i><b>1.1.2</b> Bayesian model selection</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="quick-start.html"><a href="quick-start.html#logistic-regression"><i class="fa fa-check"></i><b>1.2</b> Logistic regression</a></li>
<li class="chapter" data-level="1.3" data-path="quick-start.html"><a href="quick-start.html#non-linear-effects-via-generalized-additive-models-gams"><i class="fa fa-check"></i><b>1.3</b> Non-Linear effects via Generalized Additive Models (GAMs)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="background-bms.html"><a href="background-bms.html"><i class="fa fa-check"></i><b>2</b> Background on Bayesian model selection and averaging</a>
<ul>
<li class="chapter" data-level="2.1" data-path="background-bms.html"><a href="background-bms.html#bms-simple-example"><i class="fa fa-check"></i><b>2.1</b> A simplest example</a></li>
<li class="chapter" data-level="2.2" data-path="background-bms.html"><a href="background-bms.html#bms-framework"><i class="fa fa-check"></i><b>2.2</b> General framework</a></li>
<li class="chapter" data-level="2.3" data-path="background-bms.html"><a href="background-bms.html#bms-modelprior"><i class="fa fa-check"></i><b>2.3</b> Prior on models</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="background-bms.html"><a href="background-bms.html#binomial-prior"><i class="fa fa-check"></i><b>2.3.1</b> Binomial prior</a></li>
<li class="chapter" data-level="2.3.2" data-path="background-bms.html"><a href="background-bms.html#beta-binomial-prior"><i class="fa fa-check"></i><b>2.3.2</b> Beta-Binomial prior</a></li>
<li class="chapter" data-level="2.3.3" data-path="background-bms.html"><a href="background-bms.html#complexity-prior"><i class="fa fa-check"></i><b>2.3.3</b> Complexity prior</a></li>
<li class="chapter" data-level="2.3.4" data-path="background-bms.html"><a href="background-bms.html#a-simple-example"><i class="fa fa-check"></i><b>2.3.4</b> A simple example</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="background-bms.html"><a href="background-bms.html#bms-coefprior"><i class="fa fa-check"></i><b>2.4</b> Prior on coefficients</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="background-bms.html"><a href="background-bms.html#bms-localprior"><i class="fa fa-check"></i><b>2.4.1</b> Local priors</a></li>
<li class="chapter" data-level="2.4.2" data-path="background-bms.html"><a href="background-bms.html#bms-nlp"><i class="fa fa-check"></i><b>2.4.2</b> Non-local priors</a></li>
<li class="chapter" data-level="2.4.3" data-path="background-bms.html"><a href="background-bms.html#bms-priorvar"><i class="fa fa-check"></i><b>2.4.3</b> Sensitivity to prior variance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="background-bms.html"><a href="background-bms.html#bms-computation"><i class="fa fa-check"></i><b>2.5</b> Computation</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="background-bms.html"><a href="background-bms.html#approximating-marginal-likelihoods"><i class="fa fa-check"></i><b>2.5.1</b> Approximating marginal likelihoods</a></li>
<li class="chapter" data-level="2.5.2" data-path="background-bms.html"><a href="background-bms.html#model-search"><i class="fa fa-check"></i><b>2.5.2</b> Model search</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="background-l0.html"><a href="background-l0.html"><i class="fa fa-check"></i><b>3</b> Background on L0 criteria</a>
<ul>
<li class="chapter" data-level="3.1" data-path="background-l0.html"><a href="background-l0.html#basics"><i class="fa fa-check"></i><b>3.1</b> Basics</a></li>
<li class="chapter" data-level="3.2" data-path="background-l0.html"><a href="background-l0.html#mcmc-for-model-search"><i class="fa fa-check"></i><b>3.2</b> MCMC for model search</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>4</b> Generalized linear models</a></li>
<li class="chapter" data-level="5" data-path="gam.html"><a href="gam.html"><i class="fa fa-check"></i><b>5</b> Generalized additive models</a></li>
<li class="chapter" data-level="6" data-path="eBayes.html"><a href="eBayes.html"><i class="fa fa-check"></i><b>6</b> Empirical Bayes for transfer learning</a></li>
<li class="chapter" data-level="7" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>7</b> Survival data</a></li>
<li class="chapter" data-level="8" data-path="ggm.html"><a href="ggm.html"><i class="fa fa-check"></i><b>8</b> Gaussian graphical models</a></li>
<li class="chapter" data-level="9" data-path="mixtures.html"><a href="mixtures.html"><i class="fa fa-check"></i><b>9</b> Gaussian mixture models</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model selection with the modelSelection package</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="background-bms" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Background on Bayesian model selection and averaging<a href="background-bms.html#background-bms" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We use the term Bayesian model selection (BMS) for the prototypical setting where one considers multiple hypotheses or models and wishes to obtain posterior model probabilities for each model.
For example, in regression each model may be associated to what covariates are included in the regression equation (i.e. have non-zero coefficients), in graphical models each model may be associated to what edges are present, and in mixtures each model may correspond to a number of mixture components (clusters).</p>
<p>Section <a href="background-bms.html#bms-simple-example">2.1</a> illustrates the basic BMS notions by testing whether a Gaussian mean is zero.
Section <a href="background-bms.html#bms-framework">2.2</a> discusses the general BMS framework.
Sections <a href="background-bms.html#bms-simple-example">2.1</a>-<a href="background-bms.html#bms-framework">2.2</a> are intended for readers who are unfamiliar with BMS.
Sections <a href="background-bms.html#bms-modelprior">2.3</a> and <a href="background-bms.html#bms-coefprior">2.4</a> discuss slightly more nuanced details on how to set prior distributions, some reasonable defaults and how to induce sparsity in high dimensional problems using either sparse model priors or non-local parameter priors.
Finally, Section <a href="background-bms.html#bms-computation">2.5</a> outlines some key ideas behind computational aspects of BMS.</p>
<div id="bms-simple-example" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> A simplest example<a href="background-bms.html#bms-simple-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="example">
<p><span id="exm:normalmean" class="example"><strong>Example 2.1  </strong></span>Let <span class="math inline">\(y_i \sim N(\mu, \sigma^2)\)</span> independently for <span class="math inline">\(i=1,\ldots,n\)</span> and consider the two models (or hypotheses)
<span class="math display">\[\begin{align*}
&amp;\mu = 0 \\
&amp;\mu \neq 0.
\end{align*}\]</span>
As explained in Section <a href="background-bms.html#bms-framework">2.2</a>, in this book we denote models by <span class="math inline">\(\gamma\)</span>. In this case we use <span class="math inline">\(\gamma=0\)</span> to denote the null model <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\gamma=1\)</span> to denote the alternative model <span class="math inline">\(\mu \neq 0\)</span>.</p>
<p>The goal is to obtain the posterior probability of the alternative <span class="math inline">\(P(\mu \neq 0 \mid \mathbf{y})\)</span>, or equivalently <span class="math inline">\(P(\gamma=1 \mid \mathbf{y})\)</span>, where <span class="math inline">\(\mathbf{y}= (y_1,\ldots,y_n)\)</span> are the observed data. We may also want to obtain a BMA estimate <span class="math inline">\(E(\mu \mid \mathbf{y})=\)</span>
<span class="math display">\[E(\mu \mid \gamma=0, \mathbf{y}) P(\gamma=0 \mid \mathbf{y}) + E(\mu \mid \gamma=1, \mathbf{y}) P(\gamma=1 \mid \mathbf{y})= E(\mu \mid \gamma=1, \mathbf{y}) P(\gamma=1 \mid \mathbf{y}),\]</span>
since <span class="math inline">\(E(\mu \mid \gamma=0, \mathbf{y})=0\)</span>, and a 0.95 posterior interval for <span class="math inline">\(\mu\)</span>, that is an interval <span class="math inline">\([u_1,u_2]\)</span> such that <span class="math inline">\(P(\mu \in [u_1,u_2] \mid \mathbf{y})= 0.95\)</span>.</p>
<p>To perform a Bayesian analysis, one must specify a prior distribution on everything that is unknown. In our context:</p>
<ol style="list-style-type: decimal">
<li><p>We don’t know which model is the correct one. We hence need to specify prior model probabilities, e.g. <span class="math inline">\(P(\gamma=0)\)</span> and <span class="math inline">\(P(\gamma=1)\)</span> in this example.</p></li>
<li><p>Even if we knew the model, we don’t know the value of its parameters. We hence need to specify a prior on the parameters of each model. In this example, we must set prior densities <span class="math inline">\(p(\sigma^2 \mid \gamma=0)\)</span> and <span class="math inline">\(p(\mu, \sigma^2 \mid \gamma=1)\)</span>.</p></li>
</ol>
<p>In simple settings with two models it is customary to set a uniform model prior. That is, equal prior model probabilities <span class="math inline">\(P(\gamma=0)= P(\gamma=1)= 1/2\)</span>, unless one has strong reasons for doing otherwise (e.g. data from a related past experiment).
In Section <a href="background-bms.html#bms-modelprior">2.3</a> we discuss how to set the model prior in more advanced settings.</p>
<p>For the prior on parameters, both models feature the error variance. A popular choice is setting an inverse gamma prior <span class="math inline">\(\sigma^2 \sim IG(a/2,l/2)\)</span> under both models, for some given <span class="math inline">\(a,l&gt;0\)</span>. Typically posterior probabilities are robust to the choice of <span class="math inline">\((a,l)\)</span>, provided they’re small values (by default, <code>modelSelection</code> sets <span class="math inline">\(a=l=0.01\)</span>).
To complete the parameter prior under the alternative model we must set a prior on <span class="math inline">\(\mu\)</span> given <span class="math inline">\(\sigma^2\)</span>. A popular choice is <span class="math inline">\(p(\mu \mid \sigma^2, \gamma=1)= N(\mu; 0, g \sigma^2)\)</span>, for some given <span class="math inline">\(g&gt;0\)</span>. Posterior probabilities are sensitive to <span class="math inline">\(g\)</span>, but a common default (unit information prior) is <span class="math inline">\(g=1\)</span> and results are typically fairly robust as long as <span class="math inline">\(g\)</span> is not too different from this default.
In Section <a href="background-bms.html#bms-priorvar">2.4.3</a> we extend the example to consider a wide range of <span class="math inline">\(g\)</span> values.
See also Section <a href="background-bms.html#bms-coefprior">2.4</a> for some discussion on parameter priors in more advanced examples.</p>
</div>
<p>Let us work out Example <a href="background-bms.html#exm:normalmean">2.1</a> in R. We simulate a dataset where <span class="math inline">\(\mu=0\)</span>, set a uniform model prior and a Gaussian prior on <span class="math inline">\(\mu\)</span> with <code>normalidprior</code> setting the default <span class="math inline">\(g=1\)</span> (corresponding to argument <code>taustd</code>).
Importantly, we set the argument <code>center=FALSE</code> because otherwise <code>modelSelection</code> centers <span class="math inline">\(\mathbf{y}\)</span> by subtracting its sample mean and, while this is conventional in regression where there is little interest in the intercept, in this example it would be inappropriate (the centered <span class="math inline">\(\mathbf{y}\)</span> has mean 0 by definition!).
We obtain a high posterior probability <span class="math inline">\(P(\gamma = 0 \mid \mathbf{y})\)</span>, hence there isn’t Bayesian evidence that <span class="math inline">\(\mu \neq 0\)</span>.
We remark that for a sample size of <span class="math inline">\(n=100\)</span> and a single parameter being tested (<span class="math inline">\(\mu\)</span>), one might expect to get more conclusive evidence in favor of the null. This issue can be addressed by setting a non-local prior on <span class="math inline">\(\mu\)</span>, please see Section <a href="background-bms.html#bms-coefprior">2.4</a>.
We use <code>coef</code> to obtain the BMA estimates and 0.95 intervals for <span class="math inline">\(\mu\)</span> (row <code>Intercept</code> in the output below) and <span class="math inline">\(\sigma^2\)</span> (row <code>phi</code>).
For comparison, a t-test also doesn’t lead to rejecting the null model.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="background-bms.html#cb34-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb34-2"><a href="background-bms.html#cb34-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb34-3"><a href="background-bms.html#cb34-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb34-4"><a href="background-bms.html#cb34-4" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y=</span>y)</span></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="background-bms.html#cb35-1" tabindex="-1"></a>priorDelta <span class="ot">&lt;-</span> <span class="fu">modelunifprior</span>()</span>
<span id="cb35-2"><a href="background-bms.html#cb35-2" tabindex="-1"></a>priorCoef <span class="ot">&lt;-</span> <span class="fu">normalidprior</span>(<span class="at">taustd=</span><span class="dv">1</span>)</span>
<span id="cb35-3"><a href="background-bms.html#cb35-3" tabindex="-1"></a>ms <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>df, <span class="at">priorCoef=</span>priorCoef, <span class="at">priorDelta=</span>priorDelta, <span class="at">center=</span><span class="cn">FALSE</span>, <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb35-4"><a href="background-bms.html#cb35-4" tabindex="-1"></a><span class="fu">postProb</span>(ms)</span></code></pre></div>
<pre><code>##   modelid family        pp
## 1         normal 0.7510781
## 2       1 normal 0.2489219</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="background-bms.html#cb37-1" tabindex="-1"></a><span class="fu">coef</span>(ms)</span></code></pre></div>
<pre><code>##                estimate       2.5%    97.5%    margpp
## (Intercept) -0.03949096 -0.2848408 0.000000 0.2489219
## phi          1.03658377  0.7882084 1.365121 1.0000000</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="background-bms.html#cb39-1" tabindex="-1"></a><span class="fu">t.test</span>(y)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  y
## t = -1.5607, df = 99, p-value = 0.1218
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -0.35605755  0.04253406
## sample estimates:
##  mean of x 
## -0.1567617</code></pre>
<p>We next plot how the posterior probability and the t-test P-value change when the sample mean ranges from <span class="math inline">\(0, 0.05, 0.1, \ldots, 0.5\)</span>.
Figure <a href="background-bms.html#fig:normalmean-pp">2.1</a> shows the results.
As the sample mean of <span class="math inline">\(\mathbf{y}\)</span> grows, we obtain overwhelming evidence for <span class="math inline">\(\mu \neq 0\)</span>.
Note that the Bayesian framework is more conservative, in that one obtains a P-value &lt; 0.05 for smaller before one obtains <span class="math inline">\(p(M_2 \mid \mathbf{y}) &gt; 0.95\)</span>. This conservative character of Bayes factors is well-known, and it is one of the reasons why BMS induces sparsity in high-dimensional problems. One may obtain even more conservative results by setting certain model and parameter priors, as discussed in the next sections.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="background-bms.html#cb41-1" tabindex="-1"></a>y0 <span class="ot">&lt;-</span> y <span class="sc">-</span> <span class="fu">mean</span>(y)</span>
<span id="cb41-2"><a href="background-bms.html#cb41-2" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, .<span class="dv">5</span>, <span class="at">by=</span>.<span class="dv">05</span>)</span>
<span id="cb41-3"><a href="background-bms.html#cb41-3" tabindex="-1"></a>pp.yplus <span class="ot">&lt;-</span> pval.yplus <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="fu">length</span>(mu))</span>
<span id="cb41-4"><a href="background-bms.html#cb41-4" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(mu)) {</span>
<span id="cb41-5"><a href="background-bms.html#cb41-5" tabindex="-1"></a>  dfplus <span class="ot">&lt;-</span> <span class="fu">transform</span>(df, <span class="at">yplus=</span> y0 <span class="sc">+</span> mu[i])</span>
<span id="cb41-6"><a href="background-bms.html#cb41-6" tabindex="-1"></a>  ms <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(yplus <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>dfplus, <span class="at">priorCoef=</span>priorCoef, <span class="at">priorDelta=</span>priorDelta, <span class="at">center=</span><span class="cn">FALSE</span>, <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb41-7"><a href="background-bms.html#cb41-7" tabindex="-1"></a>  pp.yplus[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(ms)[<span class="st">&#39;(Intercept)&#39;</span>, <span class="st">&#39;margpp&#39;</span>]</span>
<span id="cb41-8"><a href="background-bms.html#cb41-8" tabindex="-1"></a>  pval.yplus[i] <span class="ot">&lt;-</span> <span class="fu">t.test</span>(dfplus<span class="sc">$</span>yplus)<span class="sc">$</span>p.value</span>
<span id="cb41-9"><a href="background-bms.html#cb41-9" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="background-bms.html#cb42-1" tabindex="-1"></a><span class="fu">plot</span>(mu, pp.yplus, <span class="at">ylab=</span><span class="st">&#39;Posterior probability&#39;</span>, <span class="at">xlab=</span><span class="st">&#39;Sample mean&#39;</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">type=</span><span class="st">&#39;l&#39;</span>)</span>
<span id="cb42-2"><a href="background-bms.html#cb42-2" tabindex="-1"></a><span class="fu">lines</span>(mu, pval.yplus, <span class="at">col=</span><span class="st">&#39;blue&#39;</span>)</span>
<span id="cb42-3"><a href="background-bms.html#cb42-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>), <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">&#39;blue&#39;</span>,<span class="st">&#39;black&#39;</span>))</span>
<span id="cb42-4"><a href="background-bms.html#cb42-4" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&#39;topleft&#39;</span>, <span class="fu">c</span>(<span class="st">&quot;Posterior probability&quot;</span>, <span class="st">&quot;P-value&quot;</span>), <span class="at">lty=</span><span class="dv">1</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;blue&quot;</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:normalmean-pp"></span>
<img src="modelSelection-book_files/figure-html/normalmean-pp-1.png" alt="Normal mean example (n=100). Posterior probability \(P(\mu \neq 0 \mid \mathbf{y})\) and t-test P-value as a function of the sample mean. The dotted lines indicate standard 0.95 and 0.05 thresholds for posterior probabilities and P-values respectively." width="672" />
<p class="caption">
Figure 2.1: Normal mean example (n=100). Posterior probability <span class="math inline">\(P(\mu \neq 0 \mid \mathbf{y})\)</span> and t-test P-value as a function of the sample mean. The dotted lines indicate standard 0.95 and 0.05 thresholds for posterior probabilities and P-values respectively.
</p>
</div>

</div>
<div id="bms-framework" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> General framework<a href="background-bms.html#bms-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a fully general setting where one considers a set of models <span class="math inline">\(\Gamma\)</span>.
We denote individual models by <span class="math inline">\(\gamma \in \Gamma\)</span>, and the parameters of that model by <span class="math inline">\(\boldsymbol{\theta}_\gamma\)</span>.
This is without loss of generality, if one has <span class="math inline">\(K\)</span> arbitrary models then one could simply set <span class="math inline">\(\gamma \in \{1, \ldots, K\}\)</span>, and the parameters could be an infinite-dimensional object such as a density function.</p>
<p>In regression settings it is convenient to relate <span class="math inline">\(\gamma\)</span> to the non-zero parameters, as done in Example <a href="background-bms.html#exm:normalmean">2.1</a>, where we had <span class="math inline">\(\gamma= \mbox{I}(\mu \neq 0)\)</span> and <span class="math inline">\(\mbox{I}()\)</span> is the indicator function.</p>
<div class="example">
<p><span id="exm:regression" class="example"><strong>Example 2.2  </strong></span>Consider a linear regression
<span class="math display">\[\begin{align*}
y_i = \sum_{j=1}^p \beta_j x_{ij} + \epsilon_i
\end{align*}\]</span>
where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> independently across <span class="math inline">\(i=1,\ldots,n\)</span>.
Suppose that we wish to consider the <span class="math inline">\(2^p\)</span> models arising from excluding/including each of the <span class="math inline">\(p\)</span> covariates.
To this end, let <span class="math inline">\(\gamma_j= \mbox{I}(\beta_j \neq 0)\)</span> be an inclusion indicator for variable <span class="math inline">\(j=1,\ldots,p\)</span>. Then we can denote an arbitrary model by <span class="math inline">\(\boldsymbol{\gamma}= (\gamma_1, \ldots,\gamma_p)\)</span>, the model space is <span class="math inline">\(\Gamma= \{0,1\}^p\)</span>, and <span class="math inline">\(\boldsymbol{\theta}_\gamma= (\boldsymbol{\beta}_\gamma, \sigma^2)\)</span>, where <span class="math inline">\(\boldsymbol{\beta}_\gamma= \{ \beta_j : \gamma_j = 1 \}\)</span> are the non-zero regression parameters under <span class="math inline">\(\boldsymbol{\gamma}\)</span>.
In such settings we use bold face notation <span class="math inline">\(\boldsymbol{\gamma}\)</span> to stress that it is a vector.</p>
</div>
<p>Given a prior model probability <span class="math inline">\(p(\gamma)\)</span> and a prior on parameters <span class="math inline">\(p(\boldsymbol{\theta}_\gamma \mid \gamma)\)</span> for every <span class="math inline">\(\gamma\)</span>, Bayes formula gives posterior model probabilities
<span class="math display" id="eq:bms-pp">\[\begin{align}
p(\gamma \mid \mathbf{y})= \frac{p(\mathbf{y} \mid \gamma) p(\gamma)}{\sum_{\gamma&#39;} p(\mathbf{y} \mid \gamma&#39;) p(\gamma&#39;)}
\tag{2.1}
\end{align}\]</span>
where <span class="math inline">\(p(\gamma)\)</span> is the prior probability of model <span class="math inline">\(\gamma\)</span> and
<span class="math display" id="eq:bms-marglhood">\[\begin{align}
p(\mathbf{y} \mid \gamma)= \int p(\mathbf{y} \mid \boldsymbol{\theta}_\gamma, \gamma) p(\boldsymbol{\theta}_\gamma \mid \gamma) d\boldsymbol{\theta}_\gamma
\tag{2.2}
\end{align}\]</span>
is the so-called <strong>integrated (or marginal) likelihood</strong>.
In <a href="background-bms.html#eq:bms-marglhood">(2.2)</a>, <span class="math inline">\(p(\mathbf{y} \mid \boldsymbol{\theta}_\gamma, \gamma)\)</span> is the likelihood function for model <span class="math inline">\(\gamma\)</span>.
For simplicity, Equation <a href="background-bms.html#eq:bms-marglhood">(2.2)</a> assumes the standard setting where <span class="math inline">\(\boldsymbol{\theta}_\gamma\)</span> follows a continuous distribution, but it can be directly extended to cases where <span class="math inline">\(\boldsymbol{\theta}_\gamma\)</span> is discrete (then the integral becomes a sum) or a mixture of discrete and continuous distribution (then it’s an integral with respect to a suitable dominating measure).</p>
<p>Intuitively,<a href="background-bms.html#eq:bms-marglhood">(2.2)</a> says that if model <span class="math inline">\(\gamma\)</span> has a large prior probability <span class="math inline">\(p(\gamma)\)</span> and a large average value of its likelihood function (with respect to the specified prior), then it has high posterior probability <span class="math inline">\(p(\gamma \mid \mathbf{y})\)</span>.</p>
<p>A related quantity are the so-called <strong>Bayes factors</strong> between models any pair of models <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\gamma&#39;\)</span>,
<span class="math display" id="eq:bms-bf">\[\begin{align}
B_{\gamma \gamma&#39;}= \frac{p(\mathbf{y} \mid \gamma)}{p(\mathbf{y} \mid \gamma&#39;)}.
\tag{2.3}
\end{align}\]</span>
Posterior model probabilities in <a href="background-bms.html#eq:bms-pp">(2.1)</a> are one-to-one functions of Bayes factors and prior model probability ratios, namely
<span class="math display">\[\begin{align*}
&amp;p(\gamma \mid \mathbf{y})= \left( 1 + \sum_{\gamma&#39; \neq \gamma} B_{\gamma&#39; \gamma} \frac{p(\gamma&#39;)}{p(\gamma)}   \right)^{-1}
\\
&amp;\frac{p(\gamma \mid \mathbf{y})}{p(\gamma&#39; \mid \mathbf{y})}= B_{\gamma \gamma&#39;} \frac{p(\gamma)}{p(\gamma&#39;)}.
\end{align*}\]</span></p>
</div>
<div id="bms-modelprior" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Prior on models<a href="background-bms.html#bms-modelprior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In simple problems like Example <a href="background-bms.html#exm:normalmean">2.1</a> where one considers only a few models, it is customary to assign equal prior probabilities
<span class="math display" id="eq:bms-modelunifprior">\[\begin{align}
p(\boldsymbol{\gamma})=  1 / |\Gamma|.
\tag{2.4}
\end{align}\]</span>
We refer to <a href="background-bms.html#eq:bms-modelunifprior">(2.4)</a> as a <strong>uniform model prior</strong>.
This prior is not recommended for problems with a moderate to large number of parameters.
To see why, consider Example <a href="background-bms.html#exm:regression">2.2</a>.
If one sets <a href="background-bms.html#eq:bms-modelunifprior">(2.4)</a>, then it is easy to see that the implied prior distribution on the model size <span class="math inline">\(\sum_{j=1}^p \gamma_j \sim \mbox{Bin}(p,1/2)\)</span>. This prior concentrates heavily on mid-size models including roughly <span class="math inline">\(p/2\)</span> covariates, and in particular it assigns very low prior probability to models including a few covariates. That is, the prior does not induce sparsity.</p>
<p>We discuss three other model priors that are popular in the regression context where <span class="math inline">\(\boldsymbol{\gamma}= (\gamma_1,\ldots,\gamma_p)\)</span>.
We denote by <span class="math inline">\(|\boldsymbol{\gamma}|_0= \sum_{j=1}^p \gamma_j\)</span> the model size, i.e. the number of non-zero regression parameters in <span class="math inline">\(\boldsymbol{\gamma}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Binomial prior, possibly combined with empirical Bayes <span class="citation">(<a href="#ref-rognon:2025empirical">Rognon-Vael and Rossell 2025</a>)</span>.</p></li>
<li><p>Beta-Binomial prior <span class="citation">(<a href="#ref-scott:2010">Scott and Berger 2010</a>)</span>.</p></li>
<li><p>Complexity prior <span class="citation">(<a href="#ref-castillo:2015">Castillo, Schmidt-Hieber, and Vaart 2015</a>)</span>.</p></li>
</ol>
<p>We found the Beta-Binomial prior to be a very good default in practice, attaining a good balance between false positive control and preserving power to detect non-zero coefficients. This is the default prior in <code>modelSelection</code> and, unless you have good reasons for doing otherwise, we suggest that you use this.
For readers who are more familiar with the frequentist literature, the Beta-Binomial prior inspired the popular Extended BIC (EBIC) criterion <span class="citation">(<a href="#ref-chen:2008">Chen and Chen 2008</a>)</span>. Roughly speaking, the model with highest posterior probability under the Beta-Binomial prior is asymptotically equivalent to the model with best EBIC.</p>
<p>We next discuss these prior in some detail. First-time readers may wish to skip these sections.</p>
<div id="binomial-prior" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Binomial prior<a href="background-bms.html#binomial-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(\gamma_j \sim \mbox{Bern}(n, \pi_j)\)</span> independently for <span class="math inline">\(j=1,\ldots,p\)</span>.
By default <code>modelSelection</code> sets <span class="math inline">\(\pi_1= \ldots = \pi_p= \pi\)</span>, and then the prior on the model size is
<span class="math display" id="eq:bms-binomprior">\[\begin{align}
p(\boldsymbol{\gamma}) &amp;= \prod_{j=1}^p \pi^{|\boldsymbol{\gamma}|_0} (1 - \pi)^{p - |\boldsymbol{\gamma}|_0}
\\
|\boldsymbol{\gamma}|_0 &amp;\sim \mbox{Bin}(n,\pi).
\tag{2.5}
\end{align}\]</span></p>
<p>Setting small <span class="math inline">\(\pi\)</span> encourages sparse solutions, but the question is what value of <span class="math inline">\(\pi\)</span> should be chosen.
A common default is to set <span class="math inline">\(\pi= c/p\)</span> for some constant <span class="math inline">\(c&gt;0\)</span>, so that prior expected model size <span class="math inline">\(E(|\boldsymbol{\gamma}|_0)= c\)</span> regardless of <span class="math inline">\(c\)</span>. Unless one has a rough idea on how many variables may have an effect, it’s unclear what <span class="math inline">\(c\)</span> should be chosen.</p>
<p>A possible strategy, implemented in function <code>modelSelection_eBayes</code>, is to set <span class="math inline">\(\pi\)</span> using empirical Bayes. Briefly, one sets
<span class="math display">\[\begin{align*}
\hat{\pi}= \arg\max_{\pi} p(\mathbf{y} \mid \pi) p(\pi)
\end{align*}\]</span>
where <span class="math inline">\(p(\pi)\)</span> is a minimally-informative prior on <span class="math inline">\(\pi\)</span> (basically, preventing extreme values like <span class="math inline">\(\pi=0\)</span> and <span class="math inline">\(\pi=1\)</span>), and <span class="math inline">\(p(\mathbf{y} \mid \pi)= \sum_{\gamma} p(y \mid \boldsymbol{\gamma}) p(\boldsymbol{\gamma} \mid \pi)\)</span> the marginal likelihood.
The idea is to learn how much sparsity is appropriate to impose to the data at hand, as an alternative to discriminately assuming strongly sparse priors.
For example, see <span class="citation">Giannone, Lenza, and Primiceri (<a href="#ref-giannone:2021">2021</a>)</span> for a discussion that sparse priors may often be inappropriate in the Social Sciences.
We refer the reader to Section <a href="eBayes.html#eBayes">6</a> for a more detailed discussion on empirical Bayes.</p>
</div>
<div id="beta-binomial-prior" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Beta-Binomial prior<a href="background-bms.html#beta-binomial-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">Scott and Berger (<a href="#ref-scott:2010">2010</a>)</span> argued for setting a uniform prior <span class="math inline">\(\pi \sim U(0,1)\)</span> and <span class="math inline">\(\gamma_j \sim \mbox{Bern}(\pi)\)</span> independently across <span class="math inline">\(j=1,\ldots,p\)</span>.
These define <span class="math inline">\(p(\pi)\)</span> and <span class="math inline">\(p(\boldsymbol{\gamma} \mid \pi)\)</span>, which imply the following marginal prior
<span class="math display" id="eq:bms-bbprior">\[\begin{align}
p(\boldsymbol{\gamma}) \propto \frac{1}{{p \choose |\boldsymbol{\gamma}|_0}}
\tag{2.6}
\end{align}\]</span>
It is a well-known result that then the model size follows a Beta-Binomial distribution, that is <span class="math inline">\(|\boldsymbol{\gamma}|_0 \sim \mbox{Beta-Binomial}(p,1,1)\)</span> (this holds basically by definition of the Beta-Binomial distribution).
We hence refer to <a href="background-bms.html#eq:bms-bbprior">(2.6)</a> as <strong>Beta-Binomial</strong> prior.
In fact, the Beta-Binomial<span class="math inline">\((p,1,1)\)</span> is simply a discrete uniform distribution in <span class="math inline">\(0,1,\ldots,p\)</span>.</p>
<p>A perhaps simpler way to think about the Beta-Binomial prior is that one sets a uniform prior on the model size <span class="math inline">\(|\boldsymbol{\gamma}|_0\)</span> (in stark contrast with the Binomial imposed by <a href="background-bms.html#eq:bms-binomprior">(2.5)</a>), and that all models of a given size have the same probability.</p>
</div>
<div id="complexity-prior" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Complexity prior<a href="background-bms.html#complexity-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">Castillo, Schmidt-Hieber, and Vaart (<a href="#ref-castillo:2015">2015</a>)</span> showed that one may obtain optimal minimax parameter estimation rates in linear regression by setting a very sparse model prior, which they referred to as <strong>Complexity prior</strong>. As a side remark, for their results to hold, one should also set a prior on parameters than has Laplace tails or thicker, which in particular rules out using Gaussian priors. For model selection purposes (as opposed to estimation), using Gaussian priors on parameters leads to good rates <span class="citation">(<a href="#ref-rossell:2022">David Rossell 2022</a>)</span>, and they’re much more convenient computationally, in particular in regression where they give closed-form marginal likelihoods in <a href="background-bms.html#eq:bms-marglhood">(2.2)</a>).
Hence <code>modelSelection</code> focuses on using Gaussian priors on parameters.</p>
<p>The main idea of a Complexity prior is that the implied prior on the model size <span class="math inline">\(P(|\boldsymbol{\gamma}|_0= l)\)</span> decreases essentially exponentially with <span class="math inline">\(l\)</span>.
Specifically, here we define <span class="math inline">\(\boldsymbol{\gamma} \sim \mbox{Complexity}(c)\)</span> for some given <span class="math inline">\(c &gt; 0\)</span> (and by default, we take <span class="math inline">\(c=1\)</span>), whenever
<span class="math display" id="eq:bms-complexprior">\[\begin{align}
p(\boldsymbol{\gamma}) \propto \frac{1}{p^{c l} {p \choose |\boldsymbol{\gamma}|_0}}
\Longrightarrow
P(|\boldsymbol{\gamma}|_0= l) \propto \frac{1}{p^{c l}}.
\tag{2.7}
\end{align}\]</span></p>
</div>
<div id="a-simple-example" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> A simple example<a href="background-bms.html#a-simple-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a regression example with <span class="math inline">\(p=2\)</span> covariates, leading to the four models shown in Table <a href="background-bms.html#tab:tabmodelprior">2.1</a>. The uniform model prior assigns 1/4 probability to each, implying a prior probabilities 1/4, 1/2 and 1/4 to model sizes 0, 1 and 2 respectively.
The Beta-Binomial prior assigns 1/3 to each model size. Since there are 2 models of size <span class="math inline">\(|\boldsymbol{\gamma}|_0=1\)</span>, each receives probability <span class="math inline">\((1/3) (1/2)= 1/6\)</span>.
The Complexity prior results in a much sparser model prior, which works great when the data-generating truth is truly sparse or the sample size <span class="math inline">\(n\)</span> is large enough, otherwise it may suffer from lower power of detecting truly non-zero parameters.</p>
<table>
<caption><span id="tab:tabmodelprior">Table 2.1: </span>Model prior probabilities in a regression example with p=2 covariates</caption>
<thead>
<tr class="header">
<th align="right">gamma1</th>
<th align="right">gamma2</th>
<th align="right">Uniform</th>
<th align="right">Beta-Binomial</th>
<th align="right">Complexity(1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.25</td>
<td align="right">0.3333333</td>
<td align="right">0.6652410</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0.25</td>
<td align="right">0.1666667</td>
<td align="right">0.1223642</td>
</tr>
<tr class="odd">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.25</td>
<td align="right">0.1666667</td>
<td align="right">0.1223642</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0.25</td>
<td align="right">0.3333333</td>
<td align="right">0.0900306</td>
</tr>
</tbody>
</table>
<p>Let us illustrate these issues in a simple simulation.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="background-bms.html#cb43-1" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">4</span>; n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb43-2"><a href="background-bms.html#cb43-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), <span class="at">nrow=</span>n)</span>
<span id="cb43-3"><a href="background-bms.html#cb43-3" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>), <span class="fu">rep</span>(<span class="fl">0.5</span>, p<span class="dv">-2</span>)), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb43-4"><a href="background-bms.html#cb43-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">%*%</span> beta <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb43-5"><a href="background-bms.html#cb43-5" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span>
<span id="cb43-6"><a href="background-bms.html#cb43-6" tabindex="-1"></a>ms.unif <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> ., <span class="at">data=</span>df, <span class="at">priorDelta =</span> <span class="fu">modelunifprior</span>(), <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb43-7"><a href="background-bms.html#cb43-7" tabindex="-1"></a>ms.bbin <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> ., <span class="at">data=</span>df, <span class="at">priorDelta =</span> <span class="fu">modelbbprior</span>(), <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb43-8"><a href="background-bms.html#cb43-8" tabindex="-1"></a>ms.comp <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y <span class="sc">~</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> ., <span class="at">data=</span>df, <span class="at">priorDelta =</span> <span class="fu">modelcomplexprior</span>(), <span class="at">verbose=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="background-bms.html#cb44-1" tabindex="-1"></a><span class="fu">coef</span>(ms.unif)</span></code></pre></div>
<pre><code>##              estimate        2.5%      97.5%     margpp
## intercept 0.010088004 -0.07671732 0.06700013 1.00000000
## X1        0.052849466  0.00000000 0.43701525 0.17309500
## X2        0.003335324  0.00000000 0.00000000 0.01986338
## X3        0.458777850  0.00000000 0.75584664 0.91440091
## X4        0.717333220  0.44869371 0.99985446 0.99952478
## phi       0.890176124  0.59739527 1.33357635 1.00000000</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="background-bms.html#cb46-1" tabindex="-1"></a><span class="fu">coef</span>(ms.bbin)</span></code></pre></div>
<pre><code>##              estimate        2.5%      97.5%     margpp
## intercept 0.007733875 -0.07805773 0.06894077 1.00000000
## X1        0.076415774  0.00000000 0.46090766 0.23952664
## X2        0.007316171  0.00000000 0.16517675 0.04047445
## X3        0.451085060  0.00000000 0.76375200 0.88959840
## X4        0.725632631  0.45322866 1.00879847 0.99904588
## phi       0.892743238  0.60243438 1.34532111 1.00000000</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="background-bms.html#cb48-1" tabindex="-1"></a><span class="fu">coef</span>(ms.comp)</span></code></pre></div>
<pre><code>##                estimate        2.5%      97.5%      margpp
## intercept -0.0175989596 -0.08065022 0.05930749 1.000000000
## X1         0.0159372331  0.00000000 0.30412930 0.050765743
## X2         0.0008506681  0.00000000 0.00000000 0.006235675
## X3         0.3067855113  0.00000000 0.71561984 0.612913816
## X4         0.7173532130  0.43773061 1.01398309 0.991811587
## phi        0.9598673704  0.62117928 1.48441620 1.000000000</code></pre>
</div>
</div>
<div id="bms-coefprior" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Prior on coefficients<a href="background-bms.html#bms-coefprior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="bms-localprior" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Local priors<a href="background-bms.html#bms-localprior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="bms-nlp" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Non-local priors<a href="background-bms.html#bms-nlp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="bms-priorvar" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Sensitivity to prior variance<a href="background-bms.html#bms-priorvar" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We return to Example <a href="background-bms.html#exm:normalmean">2.1</a>,
and assess the robustness of the results for the original data as one varies the value of the prior dispersion <span class="math inline">\(g\)</span>.
This is interesting because much literature has been devoted to the so-called Jeffreys-Lindley-Bartlett paradox <span class="citation">(<a href="#ref-lindley:1957">Lindley 1957</a>)</span>. Briefly, as <span class="math inline">\(g \to \infty\)</span> the posterior probability <span class="math inline">\(P(\mu=0 \mid \mathbf{y})\)</span> converges to 1, which a number of authors viewed as problematic.
We contend that this is not so: if one views <span class="math inline">\(g\)</span> as a tuning parameter, then <span class="math inline">\(g= \infty\)</span> is an extreme value and it’s therefore unsurprising that one obtains extreme results. For example, an infinite LASSO penalty also leads to <span class="math inline">\(\hat{\mu}=0\)</span>, yet this doesn’t stop anyone from using LASSO. The question is whether one can set tuning parameters to values that lead to good behavior, and there’s abundant theoretical and empirical evidence that <span class="math inline">\(g=1\)</span> does so.</p>
<p>Here we consider the range <span class="math inline">\(g \in [0.1, 10]\)</span>, i.e. some of these prior variances are very different from the default <span class="math inline">\(g=1\)</span>.
We do the exercise with the dataset <code>y</code> simulated in Example <a href="background-bms.html#exm:normalmean">2.1</a>, where truly <span class="math inline">\(\mu=0\)</span>, and also with another dataset <code>y1</code> where truly <span class="math inline">\(\mu=1\)</span>.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="background-bms.html#cb50-1" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y1=</span> <span class="fu">rnorm</span>(n, <span class="at">mean=</span><span class="fl">0.5</span>))</span>
<span id="cb50-2"><a href="background-bms.html#cb50-2" tabindex="-1"></a>gseq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.001</span>, <span class="dv">10</span><span class="sc">^</span><span class="dv">5</span>, <span class="at">length=</span><span class="dv">200</span>)</span>
<span id="cb50-3"><a href="background-bms.html#cb50-3" tabindex="-1"></a>pp0.gseq <span class="ot">&lt;-</span> pp1.gseq <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="fu">length</span>(gseq))</span>
<span id="cb50-4"><a href="background-bms.html#cb50-4" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(gseq)) {</span>
<span id="cb50-5"><a href="background-bms.html#cb50-5" tabindex="-1"></a>  priorCoef <span class="ot">&lt;-</span> <span class="fu">normalidprior</span>(<span class="at">taustd=</span>gseq[i])</span>
<span id="cb50-6"><a href="background-bms.html#cb50-6" tabindex="-1"></a>  ms <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>df, <span class="at">priorCoef=</span>priorCoef, <span class="at">priorDelta=</span>priorDelta, <span class="at">center=</span><span class="cn">FALSE</span>, <span class="at">verbose=</span><span class="cn">FALSE</span>)  </span>
<span id="cb50-7"><a href="background-bms.html#cb50-7" tabindex="-1"></a>  pp0.gseq[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(ms)[<span class="st">&#39;(Intercept)&#39;</span>, <span class="st">&#39;margpp&#39;</span>]</span>
<span id="cb50-8"><a href="background-bms.html#cb50-8" tabindex="-1"></a>  ms1 <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y1 <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>df1, <span class="at">priorCoef=</span>priorCoef, <span class="at">priorDelta=</span>priorDelta, <span class="at">center=</span><span class="cn">FALSE</span>, <span class="at">verbose=</span><span class="cn">FALSE</span>)  </span>
<span id="cb50-9"><a href="background-bms.html#cb50-9" tabindex="-1"></a>  pp1.gseq[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(ms1)[<span class="st">&#39;(Intercept)&#39;</span>, <span class="st">&#39;margpp&#39;</span>]</span>
<span id="cb50-10"><a href="background-bms.html#cb50-10" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="background-bms.html#cb51-1" tabindex="-1"></a><span class="fu">plot</span>(gseq, pp0.gseq, <span class="at">xlab=</span><span class="st">&#39;g&#39;</span>, <span class="at">ylab=</span><span class="st">&#39;Posterior probability&#39;</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">type=</span><span class="st">&#39;l&#39;</span>)</span>
<span id="cb51-2"><a href="background-bms.html#cb51-2" tabindex="-1"></a><span class="fu">plot</span>(gseq, pp1.gseq, <span class="at">xlab=</span><span class="st">&#39;g&#39;</span>, <span class="at">ylab=</span><span class="st">&#39;Posterior probability&#39;</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">type=</span><span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:normalmean-gseq"></span>
<img src="modelSelection-book_files/figure-html/normalmean-gseq-1.png" alt="Posterior probability \(P(\mu \neq 0 \mid \mathbf{y})\) vs. prior dispersion \(g\) in the Gaussian mean example (n=100). Left: truly \(\mu=0\). Right: truly \(\mu=0.5\)." width="50%" /><img src="modelSelection-book_files/figure-html/normalmean-gseq-2.png" alt="Posterior probability \(P(\mu \neq 0 \mid \mathbf{y})\) vs. prior dispersion \(g\) in the Gaussian mean example (n=100). Left: truly \(\mu=0\). Right: truly \(\mu=0.5\)." width="50%" />
<p class="caption">
Figure 2.2: Posterior probability <span class="math inline">\(P(\mu \neq 0 \mid \mathbf{y})\)</span> vs. prior dispersion <span class="math inline">\(g\)</span> in the Gaussian mean example (n=100). Left: truly <span class="math inline">\(\mu=0\)</span>. Right: truly <span class="math inline">\(\mu=0.5\)</span>.
</p>
</div>

<p>Figure <a href="background-bms.html#fig:normalmean-gseq">2.2</a> shows the results.
Although <span class="math inline">\(P(\mu \neq 0 \mid \mathbf{y})\)</span> decreases as <span class="math inline">\(g\)</span> grows, when truly <span class="math inline">\(\mu=0\)</span> the changes are rather small and when truly <span class="math inline">\(\mu=0.5\)</span> the changes cannot be appreciated. Overall, the conclusions are unaffected by <span class="math inline">\(g\)</span>.
Note that as <span class="math inline">\(g \to 0\)</span> we have <span class="math inline">\(P(\mu \neq 0 \mid \mathbf{y})\)</span> approaching 0.5, this is reasonable because for <span class="math inline">\(g=0\)</span> the <span class="math inline">\(N(0, g \sigma^2)\)</span> prior under the alternative (<span class="math inline">\(\gamma=1\)</span>) states that <span class="math inline">\(\mu=0\)</span>, i.e. the null and alternative hypotheses are equivalent and both receive the same posterior probability.
This effect can only be appreciated when truly <span class="math inline">\(\mu = 0\)</span>, when truly <span class="math inline">\(\mu=0.5\)</span> we would need to consider much smaller <span class="math inline">\(g\)</span>.
Just for fun, below we consider an absurdly small <span class="math inline">\(g=0.001\)</span> and an absurdly large <span class="math inline">\(g=10^6\)</span>, and even then we obtain some evidence for <span class="math inline">\(P(\mu \ne 0 \mid \mathbf{y})\)</span>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="background-bms.html#cb52-1" tabindex="-1"></a>ms1 <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y1 <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>df1, <span class="at">priorCoef=</span><span class="fu">normalidprior</span>(<span class="at">taustd=</span><span class="fl">0.001</span>), <span class="at">priorDelta=</span>priorDelta, <span class="at">center=</span><span class="cn">FALSE</span>, <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb52-2"><a href="background-bms.html#cb52-2" tabindex="-1"></a><span class="fu">coef</span>(ms1)</span></code></pre></div>
<pre><code>##               estimate        2.5%      97.5%    margpp
## (Intercept) 0.01192704 -0.03740929 0.07927645 0.5580635
## phi         0.98260934  0.65829790 1.46853024 1.0000000</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="background-bms.html#cb54-1" tabindex="-1"></a>ms1 <span class="ot">&lt;-</span> <span class="fu">modelSelection</span>(y1 <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>df1, <span class="at">priorCoef=</span><span class="fu">normalidprior</span>(<span class="at">taustd=</span><span class="dv">10</span><span class="sc">^</span><span class="dv">6</span>), <span class="at">priorDelta=</span>priorDelta, <span class="at">center=</span><span class="cn">FALSE</span>, <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb54-2"><a href="background-bms.html#cb54-2" tabindex="-1"></a><span class="fu">coef</span>(ms1)</span></code></pre></div>
<pre><code>##               estimate      2.5%     97.5%     margpp
## (Intercept) 0.02612707 0.0000000 0.4720005 0.05728419
## phi         0.97362830 0.6372641 1.4619225 1.00000000</code></pre>
</div>
</div>
<div id="bms-computation" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Computation<a href="background-bms.html#bms-computation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="approximating-marginal-likelihoods" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Approximating marginal likelihoods<a href="background-bms.html#approximating-marginal-likelihoods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The marginal likelihood in <a href="background-bms.html#eq:bms-marglhood">(2.2)</a> has a closed-form expression in some instances, mainly regression with Gaussian errors (e.g., linear regression, non-linear additive regression) with conjugate parameter priors.
Recall that the marginal likelihood for model <span class="math inline">\(\boldsymbol{\gamma}\)</span> is
<span class="math display">\[\begin{align*}
p(\mathbf{y} \mid \gamma)= \int p(\mathbf{y} \mid \boldsymbol{\theta}_\gamma, \gamma) p(\boldsymbol{\theta}_\gamma \mid \gamma) d\boldsymbol{\theta}_\gamma.
\end{align*}\]</span></p>
<p>Outside these special cases, a numerical approximation is required.
The <code>modelSelection</code> function implements some such approximations, and their use can be specified with the argument <code>method</code>.
If <code>method</code> is not specified, <code>modelSelection</code> selects a sensible default.</p>
<p>A popular strategy in the context of model selection is to use Laplace approximations: they are fairly computationally efficient, and also highly accurate as <span class="math inline">\(n\)</span> grows <span class="citation">(<a href="#ref-kass:1990">Kass, Tierney, and Kadane 1990</a>)</span>.
To use Laplace approximations, set <code>method=='Laplace'</code>.</p>
<p>Laplace approximations require finding the posterior mode (or alternatively, the MLE) <span class="math inline">\(\hat{\boldsymbol{\theta}}_\gamma\)</span> and the hessian of the log-posterior density at <span class="math inline">\(\hat{\boldsymbol{\theta}}_\gamma\)</span>. Both these quantities can be found quickly for models that feature a few parameters, but the calculations get cumbersome when:</p>
<ul>
<li><p>One considers many models, i.e. one must repeat the optimization exercise many times</p></li>
<li><p>Some models that feature many parameters have high posterior probability, and hence they’re visited often by an MCMC model search algorithm</p></li>
<li><p>The sample size <span class="math inline">\(n\)</span> is large, so evaluating gradients (or hessians) to obtain <span class="math inline">\(\hat{\boldsymbol{\theta}}_\gamma\)</span> gets costly.</p></li>
</ul>
<p>An alternative is to use approximate Laplace approximations (ALA) <span class="citation">(<a href="#ref-rossell:2021">D. Rossell, Abril, and Bhattacharya 2021</a>)</span>, which are available by using <code>method=='ALA'</code>.
Briefly, ALA approximates <span class="math inline">\(\hat{\boldsymbol{\theta}}_\gamma\)</span> by taking a single Newton-Raphson step from an initial estimate <span class="math inline">\(\hat{\boldsymbol{\theta}}_\gamma^{(0)}\)</span>. By default <span class="math inline">\(\hat{\boldsymbol{\theta}}_\gamma^{(0)}= 0\)</span> is taken, see <span class="citation">D. Rossell, Abril, and Bhattacharya (<a href="#ref-rossell:2021">2021</a>)</span> for a study of the theoretical properties of this choice. Alternatively, in <code>modelSelection</code> one may provide other
<span class="math inline">\(\hat{\boldsymbol{\theta}}_\gamma^{(0)}\)</span> with the argument <code>initpar</code>.</p>
</div>
<div id="model-search" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Model search<a href="background-bms.html#model-search" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>modelSelection</code> uses an MCMC model search, based on classical Gibbs sampling.
<code>modelSelectionGGM</code> also implements newer birth-death-swap <span class="citation">(<a href="#ref-yangyun:2016">Yang, Wainwright, and Jordan 2016</a>)</span> and locally informed thresholded (LIT) algorithms <span class="citation">(<a href="#ref-zhou_quan:2022">Zhou et al. 2022</a>)</span>. The latter are theoretically appealing in that they have been shown to be scalable to high dimensions under relatively stringent sparsity constraints. In practice simple Gibbs sampling works remarkably well, in our experience it usually attains a similar numerical accuracy when it’s run for the same clock time as birth-death-swap and LIT.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-castillo:2015" class="csl-entry">
Castillo, I., J. Schmidt-Hieber, and A. W. van der Vaart. 2015. <span>“Bayesian Linear Regression with Sparse Priors.”</span> <em>The Annals of Statistics</em> 43 (5): 1986–2018.
</div>
<div id="ref-chen:2008" class="csl-entry">
Chen, J., and Z. Chen. 2008. <span>“Extended <span>B</span>ayesian Information Criteria for Model Selection with Large Model Spaces.”</span> <em>Biometrika</em> 95 (3): 759–71.
</div>
<div id="ref-giannone:2021" class="csl-entry">
Giannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. <span>“Economic Predictions with Big Data: The Illusion of Sparsity.”</span> <em>Econometrica</em> 89 (5): 2409–37.
</div>
<div id="ref-kass:1990" class="csl-entry">
Kass, R. E., L. Tierney, and J. B. Kadane. 1990. <span>“The Validity of Posterior Expansions Based on <span>L</span>aplace’s Method.”</span> <em>Bayesian and Likelihood Methods in Statistics and Econometrics</em> 7: 473–88.
</div>
<div id="ref-lindley:1957" class="csl-entry">
Lindley, D. V. 1957. <span>“A Statistical Paradox.”</span> <em>Biometrika</em> 44: 187–92.
</div>
<div id="ref-rognon:2025empirical" class="csl-entry">
Rognon-Vael, Paul, and David Rossell. 2025. <span>“Empirical <span>B</span>ayes for Data Integration.”</span> <em>arXiv</em> 2508.08336: 1–51.
</div>
<div id="ref-rossell:2021" class="csl-entry">
Rossell, D., O. Abril, and A. Bhattacharya. 2021. <span>“Approximate <span>L</span>aplace Approximations for Scalable Model Selection.”</span> <em>Journal of the Royal Statistical Society B</em> 83 (4): 853–79.
</div>
<div id="ref-rossell:2022" class="csl-entry">
Rossell, David. 2022. <span>“Concentration of Posterior Model Probabilities and Normalized <span>L0</span> Criteria.”</span> <em>Bayesian Analysis</em> 17 (2): 565–91.
</div>
<div id="ref-scott:2010" class="csl-entry">
Scott, J. G., and J. O Berger. 2010. <span>“Bayes and Empirical <span>B</span>ayes Multiplicity Adjustment in the Variable Selection Problem.”</span> <em>The Annals of Statistics</em> 38 (5): 2587–2619.
</div>
<div id="ref-yangyun:2016" class="csl-entry">
Yang, Y., M. J. Wainwright, and M. I. Jordan. 2016. <span>“On the Computational Complexity of High-Dimensional <span>B</span>ayesian Variable Selection.”</span> <em>The Annals of Statistics</em> 44 (6): 2497–2532.
</div>
<div id="ref-zhou_quan:2022" class="csl-entry">
Zhou, Quan, Jun Yang, Dootika Vats, Gareth O Roberts, and Jeffrey S Rosenthal. 2022. <span>“Dimension-Free Mixing for High-Dimensional <span>B</span>ayesian Variable Selection.”</span> <em>Journal of the Royal Statistical Society B</em> 84 (5): 1751–84.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="quick-start.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="background-l0.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": null,
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
